{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion_classifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annychu/CTBC_JOB/blob/master/emotion_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgcTtZIPxmz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "b7591cfa-c218-415c-ce71-2292e2c73fcb"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.11.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJt6EnH5WSyM",
        "colab_type": "code",
        "outputId": "2e603fbe-4f8d-4ddc-bb71-727b6d502393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import pandas as pd\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL-0bPEYx2_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "aafdeb8f-0d66-4850-f52c-11a60e1364e9"
      },
      "source": [
        "df_train = pd.read_csv(\"01_Hoteltrain.csv\", encoding=\"utf-8\")\n",
        "df_train = df_train.drop(columns=[\"index\"])\n",
        "if len(df_train[df_train.isnull().values==True]) > 0:\n",
        "    df_train = df_train.dropna()\n",
        "\n",
        "\n",
        "# 只取384個字元做預測\n",
        "MAX_LENGTH = 384\n",
        "\n",
        "def substr(string):\n",
        "    string = string[:MAX_LENGTH]\n",
        "    return string\n",
        "\n",
        "df_train[\"review\"] = df_train[\"review\"].apply(substr)\n",
        "\n",
        "columns = [\"review\", \"label\"]\n",
        "df_train = df_train.reindex(columns=columns)\n",
        "\n",
        "# 訓練資料分成90:10的訓練和驗證集\n",
        "df_train_copy = df_train.copy()\n",
        "train_set = df_train_copy.sample(frac=0.9, random_state=0)\n",
        "test_set = df_train_copy.drop(train_set.index)\n",
        "\n",
        "train_set.to_csv(\"train.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
        "test_set.to_csv(\"test.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
        "    \n",
        "print(train_set)\n",
        "print(test_set)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 review  label\n",
            "398   房間空間還好滿寬敞的,但是朝南面挨著馬路比較吵,房間的隔音效果只能說一般.硬體設施上特別是衛...      1\n",
            "3833  感覺還可以,就是二層樓房窗外沒有風景,被一些建築物遮擋,可能是廣告之類,晚上還可以,有窗簾,...      1\n",
            "4836                          還好住的地方居然窗口是對著居民區的設施一般般床不錯      1\n",
            "4572  環境差得很.進房間就一股醜味.設施簡陋,電視不清楚,最重要的是房間價格,自己到店大堂可以還價...      0\n",
            "636             房間感覺還可以，但是洗漱用的毛巾浴巾品質不好，感覺沒有洗乾淨，房間隔音效果不好      0\n",
            "...                                                 ...    ...\n",
            "753                        房間還行，交通也方便，當然火車站周邊略顯嘈雜，總體還行。      1\n",
            "1938                             酒店不錯,已多次住過,同等價格下次還會入住.      1\n",
            "673          房間隔音效果超級差，晚上竟然能聽到外面在搓麻將和講話聲！半夜被吵醒好幾次，我暈死！！      0\n",
            "2712  酒店裝修氣味很濃，房間感覺一般，與4星有不少差距。入住期間居然不開空調，熱得要死，剛好房間又...      0\n",
            "3511  這裡的早餐是我看到的最差的一個,基本上沒什麼吃的,就看到服務員在不聽的加白粥,下次在來我是不...      1\n",
            "\n",
            "[4500 rows x 2 columns]\n",
            "                                                 review  label\n",
            "0     來往虹橋機場,絕對方便,938公共汽車直接門對門,上車睡覺就可以了(一個半小時)賓館服務不專...      0\n",
            "21    其實住美卡不是第一次，這次算我最失望的一次！一直以來我比較喜歡這個酒店，最近我覺得很差很差．...      0\n",
            "24    其實這個價格相對這個狀況的酒店已經不便宜了（360元），這個季節，這個城市亂七八糟的。但是相...      1\n",
            "25    其實這種酒店麼，如果不是因為要趕浦東機場大早的航班肯定是沒有機會住的。所以要求不能太高。晚上...      0\n",
            "63    周圍環境還可以，購物方便。房間也是不錯，當時我預定的商務間，就是考慮上網方便。但是有時候網路...      1\n",
            "...                                                 ...    ...\n",
            "4968  寶暉酒店離機場比較近，不用到羅湖一帶找酒店了酒店大堂和餐廳等裝璜都挺漂亮的，很有特色的古雅，...      1\n",
            "4969  繼續再次點評，其他都跟以前一樣，只是對於前臺的服務速度希望再快一點，多給人一點笑臉！親和力不...      1\n",
            "4971  覺得是許昌最好的酒店，性價比相當高房間大，整潔，硬體也不錯服務相當的好自助早餐也不錯離春秋廣...      1\n",
            "4978  攜程工作人員按照我的要求，給我介紹了這家酒店，我抱著試試看的心情，去住了一次，總體感覺還是非...      1\n",
            "4985  攜程服務人員電話中向我推薦了此家酒店，向我說明這是四星酒店，客戶評價都很好。住進去之後，很疑...      0\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WDAdHgux_d6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "設計一個Dataset，每次將csv裡的一筆資料轉成bert相容格式，並回傳:\n",
        "\n",
        "tokens_tensor :: 句子，包含\"[CLS]\"\n",
        "segments_tensor :: 皆為1，識別句子。\n",
        "label_tensor :: 就tensor\n",
        "\n",
        "'''\n",
        "\n",
        "class sentimentDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataPath, tokenizer):\n",
        "        self.dataPath = dataPath\n",
        "        self.traindf = pd.read_csv(dataPath, sep=\"\\t\", encoding=\"utf-8\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.len = len(self.traindf)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # 一一取出資料\n",
        "        text, label = self.traindf.iloc[idx,:].values\n",
        "        \n",
        "        # token tensor\n",
        "        word_cls = [\"[CLS]\"]\n",
        "        token = self.tokenizer.tokenize(text)\n",
        "        word_cls += token\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_cls)\n",
        "        len_ids = len(ids)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "\n",
        "        # segment tensor\n",
        "        segments_tensor = torch.tensor([1]*len_ids)\n",
        "        # label tensor\n",
        "        label_tensor = torch.tensor(label)\n",
        "\n",
        "        return tokens_tensor, segments_tensor, label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugkvSf9QyCwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "設計一個mini_batch\n",
        "\n",
        "input : sentimentDataset回傳值的集合\n",
        "\n",
        "output : \n",
        "\n",
        "tokens_tensors :: (batch_size, max_seq_len_in_batch)\n",
        "segments_tensors :: (batch_size, max_seq_len_in_batch)\n",
        "masks_tensors :: (batch_size, max_seq_len_in_batch)  # 界定自注意力範圍，1是關注，0是padding 不需要關注。\n",
        "label_tensors :: (batch_size)\n",
        "\n",
        "'''\n",
        "\n",
        "def mini_batch(sentimentSet):\n",
        "    \n",
        "    tokens_tensors = [s[0] for s in sentimentSet]\n",
        "    segments_tensors = [s[1] for s in sentimentSet]\n",
        "    label_tensors = torch.stack([s[2] for s in sentimentSet])\n",
        "\n",
        "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
        "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
        "\n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape)\n",
        "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
        "\n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6TjcQHAyFeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "複寫一個model 並加上linear & BN\n",
        "\n",
        "'''\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F \n",
        "\n",
        "class BertForSequenceClassification_sentiment(BertForSequenceClassification):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertForSequenceClassification_sentiment, self).__init__(config)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 768)\n",
        "        self.classifier2 = nn.Linear(768, 64)\n",
        "        self.classifier3 = nn.Linear(64, self.config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):    \n",
        "        \n",
        "        outputs = self.bert(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            token_type_ids = token_type_ids,\n",
        "                            position_ids = position_ids,\n",
        "                            head_mask = head_mask,\n",
        "                            # inputs_embeds = inputs_embeds,\n",
        "                            )\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        pooled_output = self.classifier(pooled_output)\n",
        "        pooled_output = self.classifier2(pooled_output)\n",
        "        logits = self.classifier3(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHmgKkQGyIN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "f794630e-71ca-4efb-8d59-53bf44e94daa"
      },
      "source": [
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
        "NUM_LABELS = 2\n",
        "\n",
        "model = BertForSequenceClassification_sentiment.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
        "\n",
        "for name, module in model.named_children():\n",
        "    if name == \"bert\":\n",
        "        for n, _ in module.named_children():\n",
        "            print(f\"{name:}:{n:}\")\n",
        "    else:\n",
        "        print(\"{:20}{}\".format(name, module))\n",
        "\n",
        "# frozen bert and train linear layers\n",
        "# for i in model.bert.parameters():\n",
        "#     i.requires_grad = False\n",
        "\n",
        "para_sum = sum(j.numel() for j in [i for i in model.parameters() if i.requires_grad == True ])\n",
        "print(\"總共要訓練的參數:{}個\".format(para_sum))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert:embeddings\n",
            "bert:encoder\n",
            "bert:pooler\n",
            "dropout             Dropout(p=0.2, inplace=False)\n",
            "classifier          Linear(in_features=768, out_features=768, bias=True)\n",
            "classifier2         Linear(in_features=768, out_features=64, bias=True)\n",
            "classifier3         Linear(in_features=64, out_features=2, bias=True)\n",
            "總共要訓練的參數:102907586個\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvRstfQsympw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "bb3fcfec-38a8-4a79-877b-a1d02dabeb4a"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "sentimentSet = sentimentDataset(\"train.tsv\", tokenizer)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "sentimentLoader = DataLoader(sentimentSet, batch_size=BATCH_SIZE, collate_fn=mini_batch)\n",
        "\n",
        "# take a look\n",
        "data = next(iter(sentimentLoader))\n",
        "\n",
        "tokens_tensors, segments_tensors, masks_tensors, label_tensors = (i for i in data)\n",
        "\n",
        "print(f\"\"\"\n",
        "tokens_tensors.shape   = {tokens_tensors.shape} \n",
        "{tokens_tensors}\n",
        "------------------------\n",
        "segments_tensors.shape = {segments_tensors.shape}\n",
        "{segments_tensors}\n",
        "------------------------\n",
        "masks_tensors.shape    = {masks_tensors.shape}\n",
        "{masks_tensors}\n",
        "------------------------\n",
        "label_ids.shape        = {label_tensors.shape}\n",
        "{label_tensors}\n",
        "\"\"\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "tokens_tensors.shape   = torch.Size([8, 273]) \n",
            "tensor([[ 101, 2791, 7279,  ..., 1762,  857, 8013],\n",
            "        [ 101, 2697, 6221,  ...,    0,    0,    0],\n",
            "        [ 101, 6917, 1962,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 6983, 2421,  ...,    0,    0,    0],\n",
            "        [ 101, 7478, 2382,  ...,    0,    0,    0],\n",
            "        [ 101, 6983, 2421,  ...,    0,    0,    0]])\n",
            "------------------------\n",
            "segments_tensors.shape = torch.Size([8, 273])\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "------------------------\n",
            "masks_tensors.shape    = torch.Size([8, 273])\n",
            "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
            "------------------------\n",
            "label_ids.shape        = torch.Size([8])\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 1])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KutaxxiNzCqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_train = []\n",
        "acc_test = []\n",
        "train_loss = []\n",
        "TIMES = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67O8jafVzDyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75983693-a039-418a-c169-4967f64d084f"
      },
      "source": [
        "'''\n",
        "\n",
        "設計一個可以批次訓練程式\n",
        "\n",
        "'''\n",
        "times = 20\n",
        "TIMES += times\n",
        "for i in range(times):\n",
        "    # training mode\n",
        "    model.train()\n",
        "\n",
        "    lr = 1e-6\n",
        "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "    # move to gpu\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    EPOCHS = 1\n",
        "    print(\"Device:{}\".format(device))\n",
        "    print(\"=\"*50 + \"Training and Testing Start\" + \"=\"*50)\n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        # 紀錄acc\n",
        "        corr = 0\n",
        "        total = 0\n",
        "\n",
        "        # 紀錄loss\n",
        "        running_loss = 0.0\n",
        "        goal_loss = 0.005\n",
        "\n",
        "        # 逐一取出\n",
        "        for data in sentimentLoader:\n",
        "\n",
        "            tokens_tensors, segments_tensors, masks_tensors, label_tensors = (i.to(device) for i in data)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            outputs = model(input_ids=tokens_tensors,\n",
        "                            token_type_ids=segments_tensors,\n",
        "                            attention_mask=masks_tensors,\n",
        "                            labels=label_tensors)\n",
        "\n",
        "            loss, logits = outputs[:2]\n",
        "            _, pred_indice = torch.max(logits.data, 1)\n",
        "\n",
        "            # 計算corr, total\n",
        "            total += label_tensors.size(0)\n",
        "            corr += (pred_indice == label_tensors).sum().item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # 疊加loss\n",
        "            running_loss += loss.item()\n",
        "        # 平均一下loss\n",
        "        aver = total / BATCH_SIZE\n",
        "        running_loss_aver = running_loss / aver\n",
        "        train_loss.append(running_loss_aver)\n",
        "        # acc\n",
        "        acc = corr / total\n",
        "        acc_train.append(acc)\n",
        "\n",
        "        print(f\"[epoch {i+1:}] loss: {running_loss_aver:.3f}  acc: {acc*100:.2f}%\")\n",
        "        \n",
        "    '''\n",
        "\n",
        "    驗證\n",
        "\n",
        "    '''\n",
        "    sentimentSet_text = sentimentDataset(\"test.tsv\", tokenizer)\n",
        "\n",
        "    BATCH_SIZE_TEXT = 200\n",
        "    sentimentLoader_text = DataLoader(sentimentSet_text, batch_size=BATCH_SIZE_TEXT, collate_fn=mini_batch)\n",
        "\n",
        "    # eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # 紀錄acc\n",
        "    corr = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 逐一取出\n",
        "        for text_data in sentimentLoader_text:\n",
        "\n",
        "            tokens_tensors, segments_tensors, masks_tensors, label_tensors = (i.to(device) for i in text_data)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(input_ids=tokens_tensors,\n",
        "                            token_type_ids=segments_tensors,\n",
        "                            attention_mask=masks_tensors\n",
        "                            )\n",
        "\n",
        "            logits = outputs[0]\n",
        "            _, pred_indice = torch.max(logits.data, 1)\n",
        "\n",
        "            # 計算corr, total\n",
        "            total += label_tensors.size(0)\n",
        "            corr += (pred_indice == label_tensors).sum().item()\n",
        "\n",
        "        # acc\n",
        "        acc = corr / total\n",
        "        acc_test.append(acc)\n",
        "    print(f\"total:{total} correct:{corr} valid_acc:{acc*100:.2f}%\")\n",
        "    print(\"=\"*50 + \"Training and Testing End\" + \"=\"*50)\n",
        "\n",
        "model_name = f\"bert_sentiment_wordmax_{MAX_LENGTH}_loss_{running_loss_aver:.3f}_lr_{lr}.pkl\"\n",
        "torch.save(model, model_name)\n",
        "         "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 1] loss: 0.449  acc: 79.58%\n",
            "total:500 correct:440 valid_acc:88.00%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 2] loss: 0.243  acc: 91.20%\n",
            "total:500 correct:439 valid_acc:87.80%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 3] loss: 0.206  acc: 92.56%\n",
            "total:500 correct:450 valid_acc:90.00%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 4] loss: 0.182  acc: 93.69%\n",
            "total:500 correct:453 valid_acc:90.60%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 5] loss: 0.165  acc: 94.22%\n",
            "total:500 correct:443 valid_acc:88.60%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 6] loss: 0.146  acc: 95.20%\n",
            "total:500 correct:448 valid_acc:89.60%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 7] loss: 0.128  acc: 95.80%\n",
            "total:500 correct:449 valid_acc:89.80%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 8] loss: 0.110  acc: 96.42%\n",
            "total:500 correct:443 valid_acc:88.60%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 9] loss: 0.102  acc: 96.64%\n",
            "total:500 correct:447 valid_acc:89.40%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 10] loss: 0.090  acc: 97.27%\n",
            "total:500 correct:452 valid_acc:90.40%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 11] loss: 0.077  acc: 97.84%\n",
            "total:500 correct:450 valid_acc:90.00%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 12] loss: 0.066  acc: 98.07%\n",
            "total:500 correct:454 valid_acc:90.80%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 13] loss: 0.061  acc: 98.24%\n",
            "total:500 correct:441 valid_acc:88.20%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 14] loss: 0.055  acc: 98.47%\n",
            "total:500 correct:451 valid_acc:90.20%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 15] loss: 0.048  acc: 98.80%\n",
            "total:500 correct:447 valid_acc:89.40%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 16] loss: 0.046  acc: 98.73%\n",
            "total:500 correct:451 valid_acc:90.20%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 17] loss: 0.041  acc: 98.93%\n",
            "total:500 correct:443 valid_acc:88.60%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 18] loss: 0.034  acc: 98.91%\n",
            "total:500 correct:446 valid_acc:89.20%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 19] loss: 0.032  acc: 99.09%\n",
            "total:500 correct:453 valid_acc:90.60%\n",
            "==================================================Training and Testing End==================================================\n",
            "Device:cuda:0\n",
            "==================================================Training and Testing Start==================================================\n",
            "[epoch 20] loss: 0.029  acc: 99.11%\n",
            "total:500 correct:452 valid_acc:90.40%\n",
            "==================================================Training and Testing End==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertForSequenceClassification_sentiment. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvZh9myKEvn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "定義一個可以input文字和可以oupput預測判斷的class。\n",
        "\n",
        "\n",
        "'''\n",
        "class sentimentModel():\n",
        "    \n",
        "    def __init__(self, text=None, tokenizerName=\"bert-base-chinese\"):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.predict_map = {0:\"Negative\", 1: \"Positive\"}\n",
        "        self.modelPath = model_name\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokenizerName)\n",
        "        self.text = text\n",
        "        \n",
        "    r'''\n",
        "    設計把一句話轉成bert相容的形式。\n",
        "    \n",
        "    input : model的路徑\n",
        "    output : model you want\n",
        "    '''\n",
        "    def loadModel(self, modelPath):\n",
        "\n",
        "        # load model and set to cuda if they are here\n",
        "        model = torch.load(modelPath, map_location=self.device)\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        print(f\"Device:{self.device}\")\n",
        "        return model\n",
        "    \n",
        "    r'''\n",
        "    設計把一句話轉成bert相容的形式。\n",
        "\n",
        "    input : 一段話，最大長度不超過80個字\n",
        "    output : \n",
        "            token_tensor : 把文字轉成電腦可以理解的方式\n",
        "            segment_tensor : 皆設為1\n",
        "            mask_tensor : self attention 關注的地方\n",
        "    '''\n",
        "    def convert_text_to_bertEat(self, text):\n",
        "\n",
        "        if type(text) != str:\n",
        "            # raise TypeError(\"Input must be str.\")\n",
        "            text = str(text)\n",
        "        elif len(text) > MAX_LENGTH:\n",
        "            # raise ValueError(\"the len(s) must less than 384.\")\n",
        "            text = text[:MAX_LENGTH]\n",
        "\n",
        "        # 取得3個tensor\n",
        "        token = self.tokenizer.tokenize(text)\n",
        "        word_cls = [\"[CLS]\"]\n",
        "        word_cls += token\n",
        "        word_cls_len = len(word_cls)\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_cls)\n",
        "        token_tensor = torch.tensor(ids)\n",
        "\n",
        "        segment_tensor = torch.tensor([1] * word_cls_len)\n",
        "        \n",
        "        mask_tensor = torch.zeros(token_tensor.shape)\n",
        "        mask_tensor = mask_tensor.masked_fill(token_tensor !=0, 1)\n",
        "\n",
        "        # covert tensor, 1D to 2D\n",
        "        token_tensor = token_tensor.view(-1,token_tensor.size(0))\n",
        "        segment_tensor = segment_tensor.view(-1,segment_tensor.size(0))\n",
        "        mask_tensor = mask_tensor.view(-1,mask_tensor.size(0))\n",
        "\n",
        "        return token_tensor, segment_tensor, mask_tensor\n",
        "\n",
        "    r'''\n",
        "    設計一個可以預測的函式，\n",
        "    input : 能被bert吃的文字\n",
        "    output : 情感結果\n",
        "    '''\n",
        "    def sentimentPredict(self, model, token_tensor, segment_tensor, mask_tensor):\n",
        "        \n",
        "        # predict mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # move all to cuda\n",
        "            if torch.cuda.is_available():\n",
        "                token_tensor = token_tensor.to(self.device).long()\n",
        "                segment_tensor = segment_tensor.to(self.device).long()\n",
        "                mask_tensor = mask_tensor.to(self.device).long()\n",
        "\n",
        "            data = [token_tensor, segment_tensor, mask_tensor]\n",
        "\n",
        "            outputs = model(*data[:3])\n",
        "            logits = outputs[0]\n",
        "\n",
        "            _, pred_num = torch.max(logits.data, 1)\n",
        "\n",
        "            pred = self.predict_map[pred_num.item()]\n",
        "            \n",
        "\n",
        "        return pred_num.item(), pred\n",
        "\n",
        "\n",
        "    r'''\n",
        "    \n",
        "    封包，流程化。\n",
        "    \n",
        "    input : 文字\n",
        "    output : 信號, 回覆的文字 \n",
        " \n",
        "    '''\n",
        "\n",
        "    def predict_process(self, text):\n",
        "        # model = self.loadModel(self.modelPath)\n",
        "\n",
        "        # print(f\"要預測的話:\\n{text}\")\n",
        "        # print(\"=\" * 25 + \"開始預測\" + \"=\" * 25)\n",
        "        token_tensor, segment_tensor, mask_tensor = self.convert_text_to_bertEat(text)\n",
        "        pred_num, pred = self.sentimentPredict(model, token_tensor, segment_tensor, mask_tensor)\n",
        "\n",
        "        # print(f\"預測結果為:{pred}\")\n",
        "        # print(\"=\" * 25 + \"預測結束\" + \"=\" * 25)\n",
        "        return pred_num  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5NtIZmSPwzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pred = pd.read_csv(\"01_Hoteltest.csv\", encoding=\"utf-8\")\n",
        "\n",
        "df_pred[\"review\"] = df_pred[\"review\"].apply(substr)\n",
        "\n",
        "columns = [\"index\", \"review\", \"label\"]\n",
        "df_pred = df_pred.reindex(columns=columns)\n",
        "\n",
        "df_pred[\"label\"] = None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV1wFybME3gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "60d5b584-d430-4fbf-ec43-eb8a5ae32bc3"
      },
      "source": [
        "emotion_class = sentimentModel()\n",
        "model = emotion_class.loadModel(emotion_class.modelPath)\n",
        "print(\"=\" * 25 + \"開始預測\" + \"=\" * 25)\n",
        "result = []\n",
        "result = df_pred[\"review\"].apply(emotion_class.predict_process)\n",
        "print(\"=\" * 25 + \"預測結束\" + \"=\" * 25)\n",
        "df_pred[\"label\"] = result\n",
        "df_pred"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:cuda:0\n",
            "=========================開始預測=========================\n",
            "=========================預測結束=========================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>\"此期間預訂，入住首日酒店贈送每間房10元洗衣券一張，通過攜程預訂，入住首日每房還可獲贈歡迎...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>&amp;#35828;&amp;#23454;&amp;#35805;，&amp;#23545;景&amp;#21306;酒店的硬...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>(1)房間衛生乾淨空間大!(2)早餐美味風富菜色多!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>（1）酒店冊子介紹說房間內提供飲用水，水壺內沒有水，給前臺提意見。前臺說飲用水就是衛生間的自...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>*房間很不錯，服務很好，就是位置偏點，在機場到市區的路邊，打車到江北商業圈起步價。*早餐不錯。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2760</th>\n",
              "      <td>2761</td>\n",
              "      <td>鷺江賓館的位置非常好，交通輪渡可謂四通八達。賓館旁邊是步行街，對面是鼓浪嶼，如果住海景房的話...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2761</th>\n",
              "      <td>2762</td>\n",
              "      <td>鹽城來了很多次，第一次住鹽阜賓館，我的確很失望整個牆壁黑咕隆咚的，好像被煙熏過一樣傢俱非常的...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>2763</td>\n",
              "      <td>觀海木樓建築別致,風景優美,環境幽靜,絕佳的度假勝地,8628房間位置極好.只是每天不是24...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>2764</td>\n",
              "      <td>觀景大床房，住了2晚。酒店位置很好，距離虹橋機場只有不到10公里，出行、叫車都算方便。據說是...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2765</td>\n",
              "      <td>鬱悶!!!氣憤!!不明白光纖竟然比上海錦江之星的網速還慢,想要晚上網速快千萬別去這家!!!因...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2765 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index                                             review  label\n",
              "0         1  \"此期間預訂，入住首日酒店贈送每間房10元洗衣券一張，通過攜程預訂，入住首日每房還可獲贈歡迎...      1\n",
              "1         2  &#35828;&#23454;&#35805;，&#23545;景&#21306;酒店的硬...      1\n",
              "2         3                         (1)房間衛生乾淨空間大!(2)早餐美味風富菜色多!      1\n",
              "3         4  （1）酒店冊子介紹說房間內提供飲用水，水壺內沒有水，給前臺提意見。前臺說飲用水就是衛生間的自...      0\n",
              "4         5    *房間很不錯，服務很好，就是位置偏點，在機場到市區的路邊，打車到江北商業圈起步價。*早餐不錯。      1\n",
              "...     ...                                                ...    ...\n",
              "2760   2761  鷺江賓館的位置非常好，交通輪渡可謂四通八達。賓館旁邊是步行街，對面是鼓浪嶼，如果住海景房的話...      1\n",
              "2761   2762  鹽城來了很多次，第一次住鹽阜賓館，我的確很失望整個牆壁黑咕隆咚的，好像被煙熏過一樣傢俱非常的...      0\n",
              "2762   2763  觀海木樓建築別致,風景優美,環境幽靜,絕佳的度假勝地,8628房間位置極好.只是每天不是24...      1\n",
              "2763   2764  觀景大床房，住了2晚。酒店位置很好，距離虹橋機場只有不到10公里，出行、叫車都算方便。據說是...      1\n",
              "2764   2765  鬱悶!!!氣憤!!不明白光纖竟然比上海錦江之星的網速還慢,想要晚上網速快千萬別去這家!!!因...      0\n",
              "\n",
              "[2765 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAFRWUmfOlvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"index\", \"label\", \"review\"]\n",
        "df_pred = df_pred.reindex(columns=columns)\n",
        "df_pred.to_csv(\"pred.csv\", index=False, encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}